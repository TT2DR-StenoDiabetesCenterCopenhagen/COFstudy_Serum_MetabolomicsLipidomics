---
title: "LASSO regression"
author: "Sofie Olund Villumsen"
date: "18/01/2022"
output: html_document
---

# https://www.r-bloggers.com/2021/05/lasso-regression-model-with-r-code/

# Ridge regression tends to do a little better, than Lasso regression when all the variables are useful variables. Lasso regression can exclude useless variables from the equation

```{r}
rm(list = ls()) # remove all files from your workspace

library(glmnet)
library(tidyverse)
```

```{r}
path <- "L:/LovbeskyttetMapper/HCOF Stem Cells/Sofie/Baseline_study/Metabolomics/"
setwd(path)

# Include only topmetabolites in the regression model to minimize the associations
top_metabolites <- c("alpha-ketoglutaric acid", "alpha-Tocopherol", "Arachidonic acid", "Cholesterol", "Citrulline", "Hippuric acid", "Leucine", "Linoleic acid", "Ornithine", "Tyrosine") 

clinical_data <- read_csv(paste(path, "data/01_clinical_data.csv", sep="/"), show_col_types = FALSE) %>% 
  select(-c(visit, id, bwcat, NAFLD, randomization, visitid, bmi_day1, age_day1)) %>% 
  colnames

combined_data <- read_csv(paste(path, "data/02_combined_data_imputed_log2_zscore.csv", sep="/"), show_col_types = FALSE) %>% 
  mutate(bwcat_binary = case_when(bwcat == "NBW" ~ "0",
                                  bwcat == "LBW" ~ "1", 
                                  TRUE ~ bwcat),
         label_binary = case_when(Label == "A" ~ "0",
                                  Label == "B" ~ "1", 
                                  Label == "C" ~ "2",
                                  TRUE ~ Label),
         randomization_binary = case_when(randomization == "Control" ~ "0",
                                          randomization == "Training" ~ "1", 
                                          TRUE ~ randomization),
         NAFLD_binary = case_when(NAFLD == "FALSE" ~ "0",
                                  NAFLD == "TRUE" ~ "1"),
         bwcat_NAFLD_binary = case_when(bwcat_NAFLD == "NBW_FALSE" ~ "0",
                                        bwcat_NAFLD == "LBW_FALSE" ~ "1", 
                                        bwcat_NAFLD == "LBW_TRUE" ~ "2",
                                        TRUE ~ bwcat_NAFLD)) %>% 
#  select(Sample_ID, bwcat_binary, label_binary, NAFLD_binary, bwcat_NAFLD_binary, randomization_binary,all_of(top_metabolites), all_of(clinical_data), -c(bwcat, Label, NAFLD, bwcat_NAFLD, randomization, visitid)) %>% #
  
    select(bwcat_binary, label_binary, bwcat_NAFLD_binary, bmi_day1, age_day1, all_of(top_metabolites), -c(all_of(clinical_data), bwcat, Label, NAFLD_binary, NAFLD, bwcat_NAFLD, randomization_binary, randomization, visitid, Sample_ID)) %>%
  
  filter(label_binary == "0") %>% # Baseine results only
  mutate(across(.cols = 1:Tyrosine, as.numeric))

X <- as.matrix(combined_data)

```


```{r}
dim(combined_data)
```


```{r}
table(is.na(X))
```


## Impute data with k-nearest neighbor
```{r}
# Convert data frame from numeric to factor
indx <- sapply(combined_data, is.numeric)
combined_data[indx] <- lapply(combined_data[indx], function(combined_data) as.factor(as.character(combined_data)))
combined_data_ <- combined_data[1:15]

# Set random seed 
if(exists(".Random.seed")) rm(.Random.seed)
library(impute) # Imputing data with KNN

# Impute with KNN function
combined_data_imputed <- impute.knn(as.matrix(combined_data_))

combined_data_df_imputed <- as_tibble(combined_data_imputed$data) 

X <- as.matrix(combined_data_df_imputed)
```




```{r}

y <- combined_data_df_imputed$bwcat_binary

family_ <- "gaussian"  # gaussian for continuous and binomial for binary

```


#——————————————–
# Model
#——————————————–
```{r}
lambda <-0.01

# standard linear regression without intercept(-1)
li.eq <- lm(y ~ X-1) 

# lasso
la.eq <- glmnet(X, y, lambda=lambda, 
                family=family_,
                intercept = F, alpha=1) 
# Ridge
ri.eq <- glmnet(X, y, lambda=lambda, 
                family=family_,
                intercept = F, alpha=0) 

#print(la.eq)
#coef(ri.eq, s = 0.1)
```


#——————————————–
# Results (lambda=0.01)
#——————————————–
```{r}
df.comp_0.01 <- data.frame(
    #beta    = beta,
    Linear  = li.eq$coefficients,
    Lasso   = la.eq$beta[,1],
    Ridge   = ri.eq$beta[,1]
)
df.comp_0.01 
```

```{r}
linear_model <- lm(bwcat_binary~ NAFLD_binary + liverfatpercent +  triglyc_day2 + ldl_day2 + cholesterol_day2 + `alpha-Tocopherol` + `Hippuric acid` + bmi_day1 + age_day1, data = combined_data_df_imputed)
summary(linear_model)
```
We want to predict whether the individual is born with a NBW or a LBW based on the following data:
- WHR_day1 + bmi_day1 + NAFLD_binary + `alpha-Tocopherol` + `Hippuric acid` + age_day1


#——————————————–
# Notes
#——————————————–
# The above and following table shows true coefficients (β) with which we generate data, the estimated coefficients of three regressoin models.
# The estimation results provide similar results between models despite the uncertainty in data-generating process. In particular, Lasso is ide λ. The lasso figure shows the change of estimated coefficients with respect to the change of the penalty parameter (log(λ)) which is the shrinkage path.
    
#——————————————–
# Results (lambda=0.1)
#——————————————–
```{r}
lambda <- 0.1

# lasso
la.eq <- glmnet(X, y, lambda=lambda,
                family=family_,
                intercept = F, alpha=1) 
# Ridge
ri.eq <- glmnet(X, y, lambda=lambda,
                family=family_,
                intercept = F, alpha=0) 

df.comp_0.1 <- data.frame(
    #beta    = beta,
    Linear  = li.eq$coefficients,
    Lasso   = la.eq$beta[,1],
    Ridge   = ri.eq$beta[,1]
)
df.comp_0.1
```

    
#————————————————
# Shrinkage of coefficients - Lasso
# (rangle lambda input or without lambda input)
#————————————————
```{r}
# lasso
la.eq <- glmnet(X, y, family=family_, 
                intercept = F, alpha=1) 
# plot 
matplot(log(la.eq$lambda), t(la.eq$beta),
               type="l", main="Lasso", lwd=2)
```

#————————————————
# Shrinkage of coefficients - Ridge
# (rangle lambda input or without lambda input)
#————————————————
```{r}
# Ridge
ri.eq <- glmnet(X, y, family=family_,  
                intercept = F, alpha=0) 

# plot
matplot(log(ri.eq$lambda), t(ri.eq$beta),
               type="l", main="Ridge", lwd=2)
```

    
#————————————————    
# Run cross-validation & select lambda
#————————————————
# The most important thing in Lasso boils down to select the optimal λ. This is determined in the process of the cross-validation. cv.glmnet() function in glmnet provides the cross-validation results with some proper range of λ. Using this output, we can draw a graph of log(λ) and MSE(measn squared error).
```{r}
mod_cv <- cv.glmnet(x=X, y=y, family=family_,
                    intercept = F, alpha=1)

# plot(log(mod_cv$lambda), mod_cv$cvm)
# cvm : The mean cross-validated error 
#     – a vector of length length(lambda)

# lambda.min : the λ at which 
# the minimal MSE is achieved.

# lambda.1se : the largest λ at which 
# the MSE is within one standard error 
# of the minimal MSE.

plot(mod_cv) 
```

# From the above figures, the first candiate is the λ at which the minimal MSE is achieved but it is likely that this model have many variables. The second is the largest λ at which the MSE is within one standard error of the minimal MSE. This is somewhat heuristic or empirical approach but have some merits for reducing the number of variables. It is typical to choose the second, MSE minimized 1se λ. But visual inspection is very important tool to find the pattern of shrinkage process.

#————————————————    
# Run cross-validation & select lambda
#————————————————
# The following result reports the estimated coefficients under the MSE minimized λ and MSE minimized 1se λ respectively.
```{r}
coef(mod_cv, c(mod_cv$lambda.min,
               mod_cv$lambda.1se))
print(paste(mod_cv$lambda.min,
            log(mod_cv$lambda.min)))
print(paste(mod_cv$lambda.1se,
            log(mod_cv$lambda.1se)))
```


#————————————————    
# Run cross-validation & select lambda
#————————————————

```{r}
mod_cv <- cv.glmnet(x=X, y=y, family=family_,
                    intercept = F, alpha=0)

#find optimal lambda value that minimizes test MSE
best_lambda <- mod_cv$lambda.min
best_lambda

#produce plot of test MSE by lambda value
plot(mod_cv)
```

```{r}
#find coefficients of best model
best_model <- glmnet(X, y, alpha = 0, lambda = best_lambda)
coef(best_model)

```

```{r}
#use fitted best model to make predictions
y_predicted <- predict(ri.eq, s = best_lambda, newx = X)

#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq
```



# The following result reports the estimated coefficients under the MSE minimized λ and MSE minimized 1se λ respectively.
```{r}
coef(mod_cv, c(mod_cv$lambda.min,
               mod_cv$lambda.1se))
print(paste(mod_cv$lambda.min,
            log(mod_cv$lambda.min)))
print(paste(mod_cv$lambda.1se,
            log(mod_cv$lambda.1se)))
```
